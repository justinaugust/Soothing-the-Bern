{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "I created a series of functions to gather the length of the data, get posts and comments from before and after Sanders' declaration for the 2020 campaign. These are saved to pandas databases and csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from time import sleep, mktime\n",
    "from pathlib import Path\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project variables\n",
    "This project is creating an NLP model comparison to detect if a post comes from the [/r/SandersforPresident](http://reddit.com/r/sandersforpresident) subreddit during the 2016 or 2020 campaign. Using the [PushShift](http://pushshift.io) API I am going to pull down posts from each time period, starting with the day he declared his candidacy. Comments and Submissions will be evaluated so different types of metadata and content will be needed for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'SandersforPresident'\n",
    "global max_data\n",
    "max_data = 500_000\n",
    "\n",
    "fields_dict = {'comment' : ['id',\n",
    "                  'parent_id',\n",
    "                  'created_utc',\n",
    "                  'body','author',\n",
    "                  'score',\n",
    "                 ],\n",
    "               'submission' : ['id',\n",
    "                     'created_utc',\n",
    "                     'title',\n",
    "                     'selftext',\n",
    "                     'author',\n",
    "                     'score',\n",
    "                     'num_comments',\n",
    "                     'stickied',]\n",
    "              }\n",
    "\n",
    "\n",
    "years = ('2020','2016')\n",
    "d_types = ('comment','submission')\n",
    "last_retrieved = {'2016' : 1430352060,\n",
    "                  '2020': 1550563200\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Functions\n",
    "### I defined a series of 4 functions to help in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_len`\n",
    "This function combines the number of rows in the data for both comments and submissions per year to calculate the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(subreddit,d_type,year):\n",
    "    \n",
    "    # Try to read in your dataset for each subreddit/d_type/year combo\n",
    "    # and get the length. If that fails, return 0 because we have no data!\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(f'datasets/{subreddit}_{d_type}_{year}.csv')\n",
    "        df.dropna(axis=0, subset=['created_utc'], inplace=True)\n",
    "        df['created_utc'] = [datetime.datetime.fromtimestamp(int(date)) for date in df['created_utc']]\n",
    "        \n",
    "        if year == '2020':\n",
    "            if (df['created_utc'].max() >= datetime.datetime.today()):\n",
    "                return(max_data)\n",
    "            return(df.shape[0])\n",
    "        elif year == '2016':\n",
    "            if (df['created_utc'].max() >= datetime.datetime.fromtimestamp(1478692800)):\n",
    "                return(max_data)\n",
    "            else:\n",
    "                return(df.shape[0])\n",
    "        \n",
    "    except: \n",
    "        return(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pre_run`\n",
    "This function creates the data files if they do not exist and sets the start times for the retrieval if it is the first running of the function. If the data files do exist, this function grabs the `created_utc.max()` from each dataset to determine where to start fetching data at, datewise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_run(subreddit, d_type, year):\n",
    "    # 'last_retrieved' is key to getting a sequential amount of data.\n",
    "    \n",
    "    # try to read the most recent timestamp from each dataset and write\n",
    "    # to the `last_retrieved` dictionary\n",
    "    try:\n",
    "        df = pd.read_csv(f'datasets/{subreddit}_{d_type}_{year}.csv')\n",
    "        last_retrieved[year] = df.created_utc.max()\n",
    "            \n",
    "    # if that doesn't work then set the dates to the date Sanders declared\n",
    "    # his candidacy in 2016 and 2020. Create the datafiles to be written to\n",
    "    # since they do not exist.\n",
    "    except:\n",
    "        pass\n",
    "    Path(f'datasets/df_{year}.csv').touch()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `run`\n",
    "This function runs the data fetching using `get_data` and loops it based on the result from `data_len` and `max_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(subreddit, d_type, fields, year):\n",
    "    \n",
    "    data_len = get_len(subreddit,d_type,year)\n",
    "    print(f'Fetching {max_data} rows of data for:\\n- Subreddit: {subreddit}\\n- d_type: {d_type}\\n- year: {year}\\n')    \n",
    "    \n",
    "    if data_len < max_data:\n",
    "            print(f'- There are only {data_len} rows of data.') \n",
    "    \n",
    "    while data_len < max_data:\n",
    "        get_data(subreddit,\n",
    "            d_type = d_type,\n",
    "            fields = fields,\n",
    "            year = year,\n",
    "             )\n",
    "        \n",
    "        data_len = get_len(subreddit,d_type,year)\n",
    "        \n",
    "        if data_len < max_data:\n",
    "            print(f'- There are now {data_len} rows of data.')\n",
    "            print('Waiting to fetch more data ', end='')\n",
    "\n",
    "            for i in range(65):\n",
    "                print(f'.', end='')\n",
    "                sleep(1)\n",
    "            print(' Done!')\n",
    "        \n",
    "    print(f'- There are {data_len} rows of data.\\n')  \n",
    "    \n",
    "    print(f'Data fetching complete for:\\n- Subreddit: {subreddit}\\n- d_type: {d_type}\\n- Year: {year}\\n\\n')\n",
    "    print(f'- Last fetched date was {last_retrieved[year]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_data`\n",
    "This function uses `requests` and the PushShift.io API to get posts and submissions 500 at a time. It also writes those results out to csv files for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subreddit,\n",
    "            d_type,\n",
    "             year,\n",
    "             **kwargs\n",
    "             ):\n",
    "        \n",
    "    base_url = \"https://api.pushshift.io/reddit/search/\" + d_type + \"/?\"\n",
    "    \n",
    "    fields = kwargs.get('fields',None)\n",
    "    \n",
    "    params = {\n",
    "        \"subreddit\" : subreddit,\n",
    "        \"size\" : 500,\n",
    "        'after': int(last_retrieved[year]),\n",
    "    }\n",
    "\n",
    "    res = requests.get(base_url,params)\n",
    "    if res.status_code != 200:\n",
    "        print(f'Error Code: {res.status_code}')\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(res.json()['data'])[fields]\n",
    "    last_retrieved[year] = df.created_utc.max()\n",
    "    \n",
    "    df.set_index('id')\n",
    "\n",
    "    try: #try to load in your existing data and then merge\n",
    "        old_df = pd.read_csv(f'datasets/{subreddit}_{d_type}_{year}.csv')\n",
    "    except:\n",
    "        old_df = pd.DataFrame(columns=fields)\n",
    "    old_df.set_index('id')\n",
    "\n",
    "    df = pd.concat([old_df,df])\n",
    "\n",
    "    #save your data to csv for the future\n",
    "    df = df[~df.duplicated(subset='id',keep='first')] \n",
    "    df.dropna(axis=0, subset=['created_utc','score'], inplace=True)\n",
    "    df.to_csv(f'datasets/{subreddit}_{d_type}_{year}.csv', index=False)      \n",
    "    df['created_utc'] = [datetime.datetime.fromtimestamp(int(date)) for date in df['created_utc']]\n",
    "    print(f'Date of last fetched data: {df.created_utc.max()}')\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching\n",
    "Below here we fetch the data, complete with output to guide us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 500000 rows of data for:\n",
      "- Subreddit: SandersforPresident\n",
      "- d_type: submission\n",
      "- year: 2016\n",
      "\n",
      "- There are 500000 rows of data.\n",
      "\n",
      "Data fetching complete for:\n",
      "- Subreddit: SandersforPresident\n",
      "- d_type: submission\n",
      "- Year: 2016\n",
      "\n",
      "\n",
      "- Last fetched date was 1483400691\n"
     ]
    }
   ],
   "source": [
    "d_type = 'submission'\n",
    "# for year in years:\n",
    "year = '2016'\n",
    "pre_run(subreddit = subreddit,\n",
    "        d_type=d_type,\n",
    "        year = year)\n",
    "\n",
    "run(subreddit,\n",
    "    d_type=d_type,\n",
    "    fields = fields_dict[d_type],\n",
    "    year = year\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here move to [Data Cleaning & Vectorization](2-Data_Cleaning_Vectorization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dsi] *",
   "language": "python",
   "name": "conda-env-.conda-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
